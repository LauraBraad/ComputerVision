{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "caea89c96714929c43ae69734c83131551e4dfa2",
    "id": "WxtVCFOFxtJJ"
   },
   "source": [
    " # Yolo v3 Object Detection in Tensorflow\n",
    " [GitHub repo link](https://github.com/heartkilla/yolo-v3) <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74c2b95e06bf3dfda575d737936438fb983b60cf",
    "id": "FWUVmwlkxtJN"
   },
   "source": [
    "<a id=\"2\"></a> \n",
    "## 2. Dependencies\n",
    "To build Yolo we're going to need Tensorflow (deep learning), NumPy (numerical computation) and Pillow (image processing) libraries. Also we're going to use seaborn's color palette for bounding boxes colors. Finally, let's import IPython function `display()` to display images in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "TLKlw7pNxtJN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 09:13:00.218111: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "TLKlw7pNxtJN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "from seaborn import color_palette\n",
    "#import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a30a24e3cc58704b009806e1c6efb1f7ca778f6",
    "id": "k1wrMWQFxtJO"
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Model hyperparameters\n",
    "Next, we define some configurations for Yolo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "8c963b6244cf03abad2defc60e7bb0b5dde7cfda",
    "id": "6cMnLn9-xtJO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_BATCH_NORM_DECAY = 0.9\n",
    "_BATCH_NORM_EPSILON = 1e-05\n",
    "_LEAKY_RELU = 0.1\n",
    "_ANCHORS = [(10, 13), (16, 30), (33, 23),\n",
    "            (30, 61), (62, 45), (59, 119),\n",
    "            (116, 90), (156, 198), (373, 326)]\n",
    "_MODEL_SIZE = (416, 416)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e88ca967006cf686c53568071030cb968359716",
    "id": "htBO3AmPxtJP"
   },
   "source": [
    "`_MODEL_SIZE` refers to the input size of the model.\n",
    "\n",
    "Let's look at other parameters step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19167dc0cd191b099b99ea63f2ce3bff02df52e4",
    "id": "cKr-DaduxtJQ"
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Model definition\n",
    "I refered to the official ResNet implementation in Tensorflow in terms of how to arange the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ca3c0f3ded8338494841e94ef11aea2c48abb82",
    "id": "hac1zGshxtJQ"
   },
   "source": [
    "### Batch norm and fixed padding\n",
    "It's useful to define `batch_norm` function since the model uses batch norms with shared parameters heavily. Also, same as ResNet, Yolo uses convolution with fixed padding, which means that padding is defined only by the size of the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "80c2ae4a51e7bc745f50997234ed9b738315b19b",
    "id": "qLtko49BxtJQ"
   },
   "outputs": [],
   "source": [
    "def batch_norm(inputs, training, data_format):\n",
    "    \"\"\"Performs a batch normalization using a standard set of parameters.\"\"\"\n",
    "    return tf.keras.layers.BatchNormalization(\n",
    "        axis=1 if data_format == 'channels_first' else 3,\n",
    "        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON,\n",
    "        scale=True)(inputs, training=training)\n",
    "\n",
    "\n",
    "def fixed_padding(inputs, kernel_size, data_format):\n",
    "    \"\"\"ResNet implementation of fixed padding.\n",
    "\n",
    "    Pads the input along the spatial dimensions independently of input size.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor input to be padded.\n",
    "        kernel_size: The kernel to be used in the conv2d or max_pool2d.\n",
    "        data_format: The input format.\n",
    "    Returns:\n",
    "        A tensor with the same format as the input.\n",
    "    \"\"\"\n",
    "    pad_total = kernel_size - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n",
    "                                        [pad_beg, pad_end],\n",
    "                                        [pad_beg, pad_end]])\n",
    "    else:\n",
    "        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n",
    "                                        [pad_beg, pad_end], [0, 0]])\n",
    "    return padded_inputs\n",
    "\n",
    "\n",
    "def conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides=1):\n",
    "    \"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n",
    "    if strides > 1:\n",
    "        inputs = fixed_padding(inputs, kernel_size, data_format)\n",
    "\n",
    "    return tf.keras.layers.Conv2D(\n",
    "        filters=filters, \n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides, \n",
    "        padding=('SAME' if strides == 1 else 'VALID'),\n",
    "        use_bias=False, \n",
    "        data_format=data_format)(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5361e186fecd4ab9c53ee37987d905f0917f18d4",
    "id": "ySw8k2AextJR"
   },
   "source": [
    "### Feature extraction: Darknet-53\n",
    "For feature extraction Yolo uses Darknet-53 neural net pretrained on ImageNet. Same as ResNet,  Darknet-53 has shortcut (residual) connections, which help information from earlier layers flow further. We omit the last 3 layers (Avgpool, Connected and Softmax) since we only need the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "d1fdc7ef8b92f0fdd60ff57911edf48cafe49d83",
    "id": "hIYrJxxNxtJR"
   },
   "outputs": [],
   "source": [
    "def darknet53_residual_block(inputs, filters, training, data_format,\n",
    "                             strides=1):\n",
    "    \"\"\"Creates a residual block for Darknet.\"\"\"\n",
    "    shortcut = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters=filters, kernel_size=1, strides=strides,\n",
    "        data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters=2 * filters, kernel_size=3, strides=strides,\n",
    "        data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs += shortcut\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def darknet53(inputs, training, data_format):\n",
    "    \"\"\"Creates Darknet53 model for feature extraction.\"\"\"\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=64, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = darknet53_residual_block(inputs, filters=32, training=training,\n",
    "                                      data_format=data_format)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=128, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(2):\n",
    "        inputs = darknet53_residual_block(inputs, filters=64,\n",
    "                                          training=training,\n",
    "                                          data_format=data_format)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=256, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(8):\n",
    "        inputs = darknet53_residual_block(inputs, filters=128,\n",
    "                                          training=training,\n",
    "                                          data_format=data_format)\n",
    "\n",
    "    route1 = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=512, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(8):\n",
    "        inputs = darknet53_residual_block(inputs, filters=256,\n",
    "                                          training=training,\n",
    "                                          data_format=data_format)\n",
    "\n",
    "    route2 = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=1024, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(4):\n",
    "        inputs = darknet53_residual_block(inputs, filters=512,\n",
    "                                          training=training,\n",
    "                                          data_format=data_format)\n",
    "\n",
    "    return route1, route2, inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1bcefa6e17216948560c6c2551197abb87b0c131",
    "id": "OJAxRM5NxtJR"
   },
   "source": [
    "### Convolution layers\n",
    "Yolo has a large number of convolutional layers. It's useful to group them in blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "42c6af6b2a1cc052b69cbb768bfdfeed60e627a0",
    "id": "wZV7NjFBxtJR"
   },
   "outputs": [],
   "source": [
    "def yolo_convolution_block(inputs, filters, training, data_format):\n",
    "    \"\"\"Creates convolution operations layer used after Darknet.\"\"\"\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    route = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    return route, inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6f6e900de2d5cd3a94d362d639fdde58f1a1707",
    "id": "Hd-GKRAQxtJS"
   },
   "source": [
    "### Detection layers\n",
    "Yolo has 3 detection layers, that detect on 3 different scales using respective anchors. For each cell in the feature map the detection layer predicts `n_anchors * (5 + n_classes)` values using 1x1 convolution. For each scale we have `n_anchors = 3`. `5 + n_classes` means that respectively to each of 3 anchors we are going to predict 4 coordinates of the box, its confidence score (the probability of containing an object) and class probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "9aa8b0d0f0e297787fe809353107ae379d1c7d24",
    "id": "zoOYAuHXxtJS"
   },
   "outputs": [],
   "source": [
    "def yolo_layer(inputs, n_classes, anchors, img_size, data_format):\n",
    "    \"\"\"Creates Yolo final detection layer.\n",
    "\n",
    "    Detects boxes with respect to anchors.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor input.\n",
    "        n_classes: Number of class labels.\n",
    "        anchors: Anchors.\n",
    "        img_size: Image size.\n",
    "        data_format: \"channels_first\" or \"channels_last\".\n",
    "\n",
    "    Returns:\n",
    "        Tensor output.\n",
    "    \"\"\"\n",
    "    n_anchors = len(anchors)\n",
    "    inputs = tf.keras.layers.Conv2D(filters=n_anchors * (5 + n_classes),\n",
    "                                    kernel_size=1, strides=1, use_bias=True,\n",
    "                                    data_format=data_format)(inputs)\n",
    "    shape = inputs.get_shape().as_list()\n",
    "    grid_shape = shape[2:4] if data_format == 'channels_first' else shape[1:3]\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.keras.layers.Reshape((n_anchors, 5 + n_classes, grid_shape[0], grid_shape[1]))(inputs)\n",
    "        strides = (img_size[0] // grid_shape[0], img_size[1] // grid_shape[1])\n",
    "    else:\n",
    "        inputs = tf.keras.layers.Reshape((grid_shape[0], grid_shape[1], n_anchors, 5 + n_classes))(inputs)\n",
    "        strides = (img_size[0] // grid_shape[0], img_size[1] // grid_shape[1])\n",
    "    box_centers, box_shapes, confidence, classes = \\\n",
    "        tf.split(inputs, (2, 2, 1, n_classes), axis=-1)\n",
    "    return box_centers, box_shapes, confidence, classes, strides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa025a0ba6b8656f12d94402d932f471b7e21e80",
    "id": "5WkKhiJExtJS"
   },
   "source": [
    "### Upsample layer\n",
    "In order to concatenate with shortcut outputs from Darknet-53 before applying detection on a different scale, we are going to upsample the feature map using nearest neighbor interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "31a240155d9f96edf06011a4b51d4cd0a999d24d",
    "id": "LBTuSae0xtJS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def upsample(inputs, out_shape, data_format):\n",
    "    \"\"\"Upsamples to `out_shape` using nearest neighbor interpolation.\"\"\"\n",
    "    new_height = out_shape[2]\n",
    "    new_width = out_shape[1]\n",
    "    inputs = tf.image.resize(inputs, (new_height, new_width), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49717e7dcca05da2c9131c67f303c7554ce7a708",
    "id": "ranHJjD2xtJT"
   },
   "source": [
    "### Non-max suppression\n",
    "The model is going to produce a lot of boxes, so we need a way to discard the boxes with low confidence scores. Also, to avoid having multiple boxes for one object, we will discard the boxes with high overlap as well using non-max suppresion for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "a32f6e7a74477dc1fe77b99ddcf72a776fea3a92",
    "id": "W8DAbXa5xtJT"
   },
   "outputs": [],
   "source": [
    "def build_boxes(inputs):\n",
    "    \"\"\"Computes top left and bottom right points of the boxes.\"\"\"\n",
    "    center_x, center_y, width, height, confidence, classes = \\\n",
    "        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)\n",
    "\n",
    "    top_left_x = center_x - width / 2\n",
    "    top_left_y = center_y - height / 2\n",
    "    bottom_right_x = center_x + width / 2\n",
    "    bottom_right_y = center_y + height / 2\n",
    "\n",
    "    boxes = tf.concat([top_left_x, top_left_y,\n",
    "                       bottom_right_x, bottom_right_y,\n",
    "                       confidence, classes], axis=-1)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def non_max_suppression(inputs, n_classes, max_output_size, iou_threshold,\n",
    "                        confidence_threshold):\n",
    "    \"\"\"Performs non-max suppression separately for each class.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor input.\n",
    "        n_classes: Number of classes.\n",
    "        max_output_size: Max number of boxes to be selected for each class.\n",
    "        iou_threshold: Threshold for the IOU.\n",
    "        confidence_threshold: Threshold for the confidence score.\n",
    "    Returns:\n",
    "        A list containing class-to-boxes dictionaries\n",
    "            for each sample in the batch.\n",
    "    \"\"\"\n",
    "    batch = tf.unstack(inputs)\n",
    "    boxes_dicts = []\n",
    "    for boxes in batch:\n",
    "        boxes = tf.boolean_mask(boxes, boxes[:, 4] > confidence_threshold)\n",
    "        classes = tf.argmax(boxes[:, 5:], axis=-1)\n",
    "        classes = tf.expand_dims(tf.to_float(classes), axis=-1)\n",
    "        boxes = tf.concat([boxes[:, :5], classes], axis=-1)\n",
    "\n",
    "        boxes_dict = dict()\n",
    "        for cls in range(n_classes):\n",
    "            mask = tf.equal(boxes[:, 5], cls)\n",
    "            mask_shape = mask.get_shape()\n",
    "            if mask_shape.ndims != 0:\n",
    "                class_boxes = tf.boolean_mask(boxes, mask)\n",
    "                boxes_coords, boxes_conf_scores, _ = tf.split(class_boxes,\n",
    "                                                              [4, 1, -1],\n",
    "                                                              axis=-1)\n",
    "                boxes_conf_scores = tf.reshape(boxes_conf_scores, [-1])\n",
    "                indices = tf.image.non_max_suppression(boxes_coords,\n",
    "                                                       boxes_conf_scores,\n",
    "                                                       max_output_size,\n",
    "                                                       iou_threshold)\n",
    "                class_boxes = tf.gather(class_boxes, indices)\n",
    "                boxes_dict[cls] = class_boxes[:, :5]\n",
    "\n",
    "        boxes_dicts.append(boxes_dict)\n",
    "\n",
    "    return boxes_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "46fde971771dab00f4061af6b0641159988fb608",
    "id": "SUPk9v-VxtJT"
   },
   "source": [
    "### Final model class\n",
    "Finally, let's define the model class using all of the layers described previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "IJnyTiiDxtJT"
   },
   "outputs": [],
   "source": [
    "class Yolo_v3:\n",
    "    \"\"\"Yolo v3 model class.\"\"\"\n",
    "\n",
    "    def __init__(self, n_classes, model_size, max_output_size, iou_threshold,\n",
    "                 confidence_threshold, data_format=None):\n",
    "        \"\"\"Creates the model.\n",
    "\n",
    "        Args:\n",
    "            n_classes: Number of class labels.\n",
    "            model_size: The input size of the model.\n",
    "            max_output_size: Max number of boxes to be selected for each class.\n",
    "            iou_threshold: Threshold for the IOU.\n",
    "            confidence_threshold: Threshold for the confidence score.\n",
    "            data_format: The input format.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        if not data_format:\n",
    "            if tf.test.is_built_with_cuda():\n",
    "                data_format = 'channels_first'\n",
    "            else:\n",
    "                data_format = 'channels_last'\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.model_size = model_size\n",
    "        self.max_output_size = max_output_size\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.data_format = data_format\n",
    "\n",
    "    def __call__(self, inputs, training):\n",
    "        \"\"\"Add operations to detect boxes for a batch of input images.\n",
    "\n",
    "        Args:\n",
    "            inputs: A Tensor representing a batch of input images.\n",
    "            training: A boolean, whether to use in training or inference mode.\n",
    "\n",
    "        Returns:\n",
    "            A list containing class-to-boxes dictionaries\n",
    "                for each sample in the batch.\n",
    "        \"\"\"\n",
    "        with tf.compat.v1.variable_scope('yolo_v3_model'):\n",
    "            if self.data_format == 'channels_first':\n",
    "                inputs = tf.compat.v1.transpose(inputs, [0, 3, 1, 2])\n",
    "\n",
    "            inputs = inputs / 255\n",
    "\n",
    "            route1, route2, inputs = darknet53(inputs, training=training,\n",
    "                                               data_format=self.data_format)\n",
    "\n",
    "            route, inputs = yolo_convolution_block(\n",
    "                inputs, filters=512, training=training,\n",
    "                data_format=self.data_format)\n",
    "            detect1 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[6:9],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "            inputs = conv2d_fixed_padding(route, filters=256, kernel_size=1,\n",
    "                                          data_format=self.data_format)\n",
    "            inputs = batch_norm(inputs, training=training,\n",
    "                                data_format=self.data_format)\n",
    "            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "            upsample_size = route2.get_shape().as_list()\n",
    "            inputs = upsample(inputs, out_shape=upsample_size,\n",
    "                              data_format=self.data_format)\n",
    "            axis = 1 if self.data_format == 'channels_first' else 3\n",
    "            inputs = tf.concat([inputs, route2], axis=axis)\n",
    "            route, inputs = yolo_convolution_block(\n",
    "                inputs, filters=256, training=training,\n",
    "                data_format=self.data_format)\n",
    "            detect2 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[3:6],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "            inputs = conv2d_fixed_padding(route, filters=128, kernel_size=1,\n",
    "                                          data_format=self.data_format)\n",
    "            inputs = batch_norm(inputs, training=training,\n",
    "                                data_format=self.data_format)\n",
    "            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "            upsample_size = route1.get_shape().as_list()\n",
    "            inputs = upsample(inputs, out_shape=upsample_size,\n",
    "                              data_format=self.data_format)\n",
    "            inputs = tf.concat([inputs, route1], axis=axis)\n",
    "            route, inputs = yolo_convolution_block(\n",
    "                inputs, filters=128, training=training,\n",
    "                data_format=self.data_format)\n",
    "            detect3 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[0:3],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "            inputs = tf.concat([detect1, detect2, detect3], axis=1)\n",
    "\n",
    "            inputs = build_boxes(inputs)\n",
    "\n",
    "            boxes_dicts = non_max_suppression(\n",
    "                inputs, n_classes=self.n_classes,\n",
    "                max_output_size=self.max_output_size,\n",
    "                iou_threshold=self.iou_threshold,\n",
    "                confidence_threshold=self.confidence_threshold)\n",
    "\n",
    "            return boxes_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cddc02edf2dbbefe566666d557881faa8b59155e",
    "id": "v2PUds_4xtJU"
   },
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Utility functions\n",
    "Here are some utility functions that will help us load images as NumPy arrays, load class names from the official file and draw the predicted boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "b992d6d649fdcb4c279deee99b2f66362066646c",
    "id": "fMEtJx9YxtJU"
   },
   "outputs": [],
   "source": [
    "def load_images(img_names, model_size):\n",
    "    \"\"\"Loads images in a 4D array.\n",
    "\n",
    "    Args:\n",
    "        img_names: A list of images names.\n",
    "        model_size: The input size of the model.\n",
    "        data_format: A format for the array returned\n",
    "            ('channels_first' or 'channels_last').\n",
    "\n",
    "    Returns:\n",
    "        A 4D NumPy array.\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "\n",
    "    for img_name in img_names:\n",
    "        img = Image.open(img_name)\n",
    "        img = img.resize(size=model_size)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.concatenate(imgs)\n",
    "\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def load_class_names(file_name):\n",
    "    \"\"\"Returns a list of class names read from `file_name`.\"\"\"\n",
    "    with open(file_name, 'r') as f:\n",
    "        class_names = f.read().splitlines()\n",
    "    return class_names\n",
    "\n",
    "\n",
    "def draw_boxes(img_names, boxes_dicts, class_names, model_size):\n",
    "    \"\"\"Draws detected boxes.\n",
    "\n",
    "    Args:\n",
    "        img_names: A list of input images names.\n",
    "        boxes_dict: A class-to-boxes dictionary.\n",
    "        class_names: A class names list.\n",
    "        model_size: The input size of the model.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    colors = ((np.array(color_palette(\"hls\", 80)) * 255)).astype(np.uint8)\n",
    "    for num, img_name, boxes_dict in zip(range(len(img_names)), img_names,\n",
    "                                         boxes_dicts):\n",
    "        img = Image.open(img_name)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        font = ImageFont.truetype(font='../input/futur.ttf',\n",
    "                                  size=(img.size[0] + img.size[1]) // 100)\n",
    "        resize_factor = \\\n",
    "            (img.size[0] / model_size[0], img.size[1] / model_size[1])\n",
    "        for cls in range(len(class_names)):\n",
    "            boxes = boxes_dict[cls]\n",
    "            if np.size(boxes) != 0:\n",
    "                color = colors[cls]\n",
    "                for box in boxes:\n",
    "                    xy, confidence = box[:4], box[4]\n",
    "                    xy = [xy[i] * resize_factor[i % 2] for i in range(4)]\n",
    "                    x0, y0 = xy[0], xy[1]\n",
    "                    thickness = (img.size[0] + img.size[1]) // 200\n",
    "                    for t in np.linspace(0, 1, thickness):\n",
    "                        xy[0], xy[1] = xy[0] + t, xy[1] + t\n",
    "                        xy[2], xy[3] = xy[2] - t, xy[3] - t\n",
    "                        draw.rectangle(xy, outline=tuple(color))\n",
    "                    text = '{} {:.1f}%'.format(class_names[cls],\n",
    "                                               confidence * 100)\n",
    "                    text_size = draw.textsize(text, font=font)\n",
    "                    draw.rectangle(\n",
    "                        [x0, y0 - text_size[1], x0 + text_size[0], y0],\n",
    "                        fill=tuple(color))\n",
    "                    draw.text((x0, y0 - text_size[1]), text, fill='black',\n",
    "                              font=font)\n",
    "\n",
    "        display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13f8a2f689aff3e5941e94343a2f060f9049917f",
    "id": "tHdzKOKixtJU"
   },
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. Converting weights to Tensorflow format\n",
    "Now it's time to load the official weights. We are going to iterate through the file and gradually create `tf.assign` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "24d4689103695a95a2634c4d688475ba2750fdea",
    "id": "krNzsLTyxtJU"
   },
   "outputs": [],
   "source": [
    "def load_weights(variables, file_name):\n",
    "    \"\"\"Reshapes and loads official pretrained Yolo weights.\n",
    "\n",
    "    Args:\n",
    "        variables: A list of tf.Variable to be assigned.\n",
    "        file_name: A name of a file containing weights.\n",
    "\n",
    "    Returns:\n",
    "        A list of assign operations.\n",
    "    \"\"\"\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        # Skip first 5 values containing irrelevant info\n",
    "        np.fromfile(f, dtype=np.int32, count=5)\n",
    "        weights = np.fromfile(f, dtype=np.float32)\n",
    "\n",
    "        assign_ops = []\n",
    "        ptr = 0\n",
    "\n",
    "        # Load weights for Darknet part.\n",
    "        # Each convolution layer has batch normalization.\n",
    "        for i in range(52):\n",
    "            conv_var = variables[5 * i]\n",
    "            gamma, beta, mean, variance = variables[5 * i + 1:5 * i + 5]\n",
    "            batch_norm_vars = [beta, gamma, mean, variance]\n",
    "\n",
    "            for var in batch_norm_vars:\n",
    "                shape = var.shape.as_list()\n",
    "                num_params = np.prod(shape)\n",
    "                var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "                ptr += num_params\n",
    "                assign_ops.append(tf.assign(var, var_weights))\n",
    "\n",
    "            shape = conv_var.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape(\n",
    "                (shape[3], shape[2], shape[0], shape[1]))\n",
    "            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(conv_var, var_weights))\n",
    "\n",
    "        # Loading weights for Yolo part.\n",
    "        # 7th, 15th and 23rd convolution layer has biases and no batch norm.\n",
    "        ranges = [range(0, 6), range(6, 13), range(13, 20)]\n",
    "        unnormalized = [6, 13, 20]\n",
    "        for j in range(3):\n",
    "            for i in ranges[j]:\n",
    "                current = 52 * 5 + 5 * i + j * 2\n",
    "                conv_var = variables[current]\n",
    "                gamma, beta, mean, variance =  \\\n",
    "                    variables[current + 1:current + 5]\n",
    "                batch_norm_vars = [beta, gamma, mean, variance]\n",
    "\n",
    "                for var in batch_norm_vars:\n",
    "                    shape = var.shape.as_list()\n",
    "                    num_params = np.prod(shape)\n",
    "                    var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "                    ptr += num_params\n",
    "                    assign_ops.append(tf.assign(var, var_weights))\n",
    "\n",
    "                shape = conv_var.shape.as_list()\n",
    "                num_params = np.prod(shape)\n",
    "                var_weights = weights[ptr:ptr + num_params].reshape(\n",
    "                    (shape[3], shape[2], shape[0], shape[1]))\n",
    "                var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "                ptr += num_params\n",
    "                assign_ops.append(tf.assign(conv_var, var_weights))\n",
    "\n",
    "            bias = variables[52 * 5 + unnormalized[j] * 5 + j * 2 + 1]\n",
    "            shape = bias.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(bias, var_weights))\n",
    "\n",
    "            conv_var = variables[52 * 5 + unnormalized[j] * 5 + j * 2]\n",
    "            shape = conv_var.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape(\n",
    "                (shape[3], shape[2], shape[0], shape[1]))\n",
    "            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(conv_var, var_weights))\n",
    "\n",
    "    return assign_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "41bed33dcd8624518a94e3aca8f96a1891fad0c5",
    "id": "sOhtYxj-xtJW"
   },
   "source": [
    "### Detections\n",
    "Testing the model with IoU (Interception over Union ratio used in non-max suppression) threshold and confidence threshold both set to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "40b4a8d68a58cc2c2fcccf0dc24345f92ae955c2",
    "id": "VWtha5AextJW"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(img_names)\n\u001b[1;32m      2\u001b[0m batch \u001b[38;5;241m=\u001b[39m load_images(img_names, model_size\u001b[38;5;241m=\u001b[39m_MODEL_SIZE)\n\u001b[1;32m      3\u001b[0m class_names \u001b[38;5;241m=\u001b[39m load_class_names(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./input/coco.names\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_names' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = len(img_names)\n",
    "batch = load_images(img_names, model_size=_MODEL_SIZE)\n",
    "class_names = load_class_names('./input/coco.names')\n",
    "n_classes = len(class_names)\n",
    "max_output_size = 10\n",
    "iou_threshold = 0.5\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "model = Yolo_v3(n_classes=n_classes, model_size=_MODEL_SIZE,\n",
    "                max_output_size=max_output_size,\n",
    "                iou_threshold=iou_threshold,\n",
    "                confidence_threshold=confidence_threshold)\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [batch_size, 416, 416, 3])\n",
    "\n",
    "detections = model(inputs, training=False)\n",
    "\n",
    "model_vars = tf.global_variables(scope='yolo_v3_model')\n",
    "assign_ops = load_weights(model_vars, './input/yolov3.weights')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(assign_ops)\n",
    "    detection_result = sess.run(detections, feed_dict={inputs: batch})\n",
    "    \n",
    "draw_boxes(img_names, detection_result, class_names, _MODEL_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vores kode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image  # Import the Image module from PIL\n",
    "\n",
    "\n",
    "# Path to the text file containing image paths\n",
    "\n",
    "\n",
    "text_file_path = \"/Users/laurabraadrasmussen/archive/coco25k.txt\"\n",
    "\n",
    "with open(text_file_path, \"r\") as file:\n",
    "    image_paths = file.readlines()\n",
    "    \n",
    "# Strip newline characters from the end of each path\n",
    "image_paths = [os.path.join(\"/Users/laurabraadrasmussen/archive\", path.strip()) for path in image_paths]\n",
    "\n",
    "# Open and display each image\n",
    "for path in image_paths[:3]:\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        #img.show()  # Display the image (you may replace this with any other processing)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening image at path {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "183546it [02:55, 1046.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import csv\n",
    "\n",
    "# Path to the text file containing image paths\n",
    "text_file_path = \"/Users/laurabraadrasmussen/archive/coco25k.txt\"\n",
    "\n",
    "# Path to the CSV file containing image names and labels\n",
    "csv_file_path = \"/Users/laurabraadrasmussen/archive/coco_minitrain2017.csv\"\n",
    "\n",
    "# Read image paths from the text file\n",
    "with open(text_file_path, \"r\") as file:\n",
    "    image_paths = file.readlines()\n",
    "\n",
    "# Strip newline characters from the end of each path\n",
    "image_paths = [os.path.join(\"/Users/laurabraadrasmussen/archive\", path.strip()) for path in image_paths]\n",
    "\n",
    "# Read image names and labels from the CSV file\n",
    "image_label_pairs = []\n",
    "with open(csv_file_path, \"r\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in tqdm(csvreader):\n",
    "        # Assuming each row of CSV contains [image_name, label]\n",
    "        image_name,a,b,c,d,label, e = row\n",
    "        # Find the corresponding image path using the image name\n",
    "        for path in image_paths:\n",
    "            if image_name in path:\n",
    "                image_label_pairs.append((path, label))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Path: /Users/laurabraadrasmussen/archive/./images/000000131075.jpg\n",
      "Label: tv\n",
      "Image Path: /Users/laurabraadrasmussen/archive/./images/000000131075.jpg\n",
      "Label: laptop\n",
      "Image Path: /Users/laurabraadrasmussen/archive/./images/000000131075.jpg\n",
      "Label: laptop\n",
      "Image Path: /Users/laurabraadrasmussen/archive/./images/000000131075.jpg\n",
      "Label: chair\n",
      "Image Path: /Users/laurabraadrasmussen/archive/./images/000000131075.jpg\n",
      "Label: tv\n",
      "Image Path: /Users/laurabraadrasmussen/archive/./images/000000393223.jpg\n",
      "Label: toothbrush\n",
      "Image Path: /Users/laurabraadrasmussen/archive/./images/000000393223.jpg\n",
      "Label: person\n",
      "Image Path: /Users/laurabraadrasmussen/archive/./images/000000393228.jpg\n",
      "Label: elephant\n",
      "Image Path: /Users/laurabraadrasmussen/archive/./images/000000393228.jpg\n",
      "Label: elephant\n",
      "Image Path: /Users/laurabraadrasmussen/archive/./images/000000393228.jpg\n",
      "Label: elephant\n"
     ]
    }
   ],
   "source": [
    "# Open and process each image with its corresponding label\n",
    "for image_path, label in image_label_pairs[:10]:\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        # Process the image with its label here\n",
    "        print(\"Image Path:\", image_path)\n",
    "        print(\"Label:\", label)\n",
    "        #img.show()  # Uncomment this line if you want to display the image\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening image at path {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = image_label_pairs[:10]\n",
    "\n",
    "img_names = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through the data and split into paths and labels\n",
    "for path, label in data:\n",
    "    img_names.append(path)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensors in list passed to 'values' of 'ConcatV2' Op have types [<NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>] that don't all match.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m Yolo_v3(n_classes\u001b[38;5;241m=\u001b[39mn_classes, model_size\u001b[38;5;241m=\u001b[39m_MODEL_SIZE,\n\u001b[1;32m     10\u001b[0m                 max_output_size\u001b[38;5;241m=\u001b[39mmax_output_size,\n\u001b[1;32m     11\u001b[0m                 iou_threshold\u001b[38;5;241m=\u001b[39miou_threshold,\n\u001b[1;32m     12\u001b[0m                 confidence_threshold\u001b[38;5;241m=\u001b[39mconfidence_threshold)\n\u001b[1;32m     14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mfloat32, shape \u001b[38;5;241m=\u001b[39m [batch_size, \u001b[38;5;241m416\u001b[39m, \u001b[38;5;241m416\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m---> 16\u001b[0m detections \u001b[38;5;241m=\u001b[39m model(inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m model_vars \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mglobal_variables(scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo_v3_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m assign_ops \u001b[38;5;241m=\u001b[39m load_weights(model_vars, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./input/yolov3.weights\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 95\u001b[0m, in \u001b[0;36mYolo_v3.__call__\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     87\u001b[0m route, inputs \u001b[38;5;241m=\u001b[39m yolo_convolution_block(\n\u001b[1;32m     88\u001b[0m     inputs, filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[1;32m     89\u001b[0m     data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format)\n\u001b[1;32m     90\u001b[0m detect3 \u001b[38;5;241m=\u001b[39m yolo_layer(inputs, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes,\n\u001b[1;32m     91\u001b[0m                      anchors\u001b[38;5;241m=\u001b[39m_ANCHORS[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m     92\u001b[0m                      img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_size,\n\u001b[1;32m     93\u001b[0m                      data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format)\n\u001b[0;32m---> 95\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([detect1, detect2, detect3], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     97\u001b[0m inputs \u001b[38;5;241m=\u001b[39m build_boxes(inputs)\n\u001b[1;32m     99\u001b[0m boxes_dicts \u001b[38;5;241m=\u001b[39m non_max_suppression(\n\u001b[1;32m    100\u001b[0m     inputs, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes,\n\u001b[1;32m    101\u001b[0m     max_output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_output_size,\n\u001b[1;32m    102\u001b[0m     iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miou_threshold,\n\u001b[1;32m    103\u001b[0m     confidence_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfidence_threshold)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/op_def_library.py:500\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m that do not match type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred from earlier arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    499\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 500\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m that don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt all match.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m that are invalid. Tensors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensors in list passed to 'values' of 'ConcatV2' Op have types [<NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>] that don't all match."
     ]
    }
   ],
   "source": [
    "batch_size = len(img_names)\n",
    "batch = load_images(img_names, model_size=_MODEL_SIZE)\n",
    "class_names = np.unique(labels)\n",
    "n_classes = len(class_names)\n",
    "max_output_size = 10\n",
    "iou_threshold = 0.5\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "model = Yolo_v3(n_classes=n_classes, model_size=_MODEL_SIZE,\n",
    "                max_output_size=max_output_size,\n",
    "                iou_threshold=iou_threshold,\n",
    "                confidence_threshold=confidence_threshold)\n",
    "\n",
    "inputs = tf.compat.v1.placeholder(tf.float32, shape = [batch_size, 416, 416, 3])\n",
    "\n",
    "detections = model(inputs, training=False)\n",
    "\n",
    "model_vars = tf.global_variables(scope='yolo_v3_model')\n",
    "assign_ops = load_weights(model_vars, './input/yolov3.weights')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(assign_ops)\n",
    "    detection_result = sess.run(detections, feed_dict={inputs: batch})\n",
    "    \n",
    "draw_boxes(img_names, detection_result, class_names, _MODEL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the YOLOv3 model\n",
    "yolo_model = tf.keras.models.load_model(\"yolov3_model.h5\")  # Replace \"yolov3_model.h5\" with the path to your YOLOv3 model\n",
    "\n",
    "# Define classes (if available)\n",
    "classes = [\"tv\", \"laptop\", \"chair\", \"toothbrush\", \"person\", \"elephant\"]  # Adjust as per your dataset\n",
    "\n",
    "# Iterate through each image and perform object detection\n",
    "for image_path, label in zip(paths, labels):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Preprocess image (if required)\n",
    "\n",
    "    # Perform object detection\n",
    "    detections = yolo_model.predict(np.expand_dims(image, axis=0))\n",
    "\n",
    "    # Postprocess the results\n",
    "    # Process the output of YOLOv3 to extract bounding boxes and class labels.\n",
    "    for detection in detections:\n",
    "        # Extract bounding box coordinates, class label, and confidence score\n",
    "        # Postprocess the detections here\n",
    "        # For example, assuming YOLOv3 output format [xmin, ymin, xmax, ymax, confidence, class_id]\n",
    "        for bbox in detection:\n",
    "            xmin, ymin, xmax, ymax, confidence, class_id = bbox\n",
    "\n",
    "            # Convert class_id to class label\n",
    "            class_label = classes[int(class_id)]\n",
    "\n",
    "            # Print the detected object and its confidence score\n",
    "            print(\"Detected:\", class_label, \"Confidence:\", confidence)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
